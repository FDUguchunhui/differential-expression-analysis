set.seed(123)
n.burnin = 500
n.iter = 5500
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
# toggle this on to experiment with large n
#y = rnorm(n=1000,mean = 4,sd=2)
n= length(y)
sigmasq.save = rep (NA, n.iter)
mu.save = rep(NA, n.iter)
# hyperparameters for mu:
mu0=0
tausq= 25^2
# hyperparameters for sigmasq:
#a=1000*2 #if these values are large, become informative. prior mean may be approximately equal, but large difference in effects...
#b=1000*1
a=2
b=1
b/(a-1) # prior mean for sigmasq
# note: small values are "uninformative", but still impact estimate for small n
#initial values
mu = 0
sigmasq = 10
for (i in 1:n.iter){
#update mu
# calculate conditional dist of mu given sigmasq and data
# update variance:
sigmasqk = 1 / (n/sigmasq+1/tausq)
# update mean:
meank = (mean(y)/(sigmasq/n) + mu0/tausq)*sigmasqk
mu = rnorm (1, meank, sqrt(sigmasqk))
mu.save[i] = mu
# update sigmasq
# calculate conditional dist of sigmasq given mu and data
RRR = y - mu
sigmasq = rinvgamma(1,shape=n/2+a,scale=sum(RRR^2)/2 + b) # note: the shape parameter does not change; see slide 33
sigmasq.save[i] = sigmasq
}
# estimate of posterior mean for mu:
mu.sample = mu.save[(n.burnin+1):n.iter]
sigmasq.sample = sigmasq.save[(n.burnin+1):n.iter]
mean(mu.sample)
mean(y) # compare to mle for mu
# estimate of posterior mean for sigmasq
mean(sigmasq.sample)
sum((y - mean(y))^2)/n # compare to sample variance, even with uninformative priors, will be different when n is small
par (mfrow = c(2,1))
plot (mu.save, type = "l", xlab = "Iteration",  ylab = expression(mu), cex.lab = 1, col = "blue", main = expression(paste("Population Mean   ", mu)))
plot (sigmasq.save, type = "l", xlab = "Iteration",  ylab = expression(sigma^2), cex.lab = 1, col = "blue", main =  expression(paste("Population Variance   ", sigma^2)) )
scatterhist = function(x, y, xlab="", ylab=""){
zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
xhist = hist(x, plot=FALSE)
yhist = hist(y, plot=FALSE)
top = max(c(xhist$counts, yhist$counts))
par(mar=c(3,3,1,1))
plot(x,y,col = "#0000ff22")
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0)
par(mar=c(3,0,1,1))
barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE)
par(oma=c(3,3,0,0))
mtext(xlab, side=1, line=1, outer=TRUE, adj=0,
at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
mtext(ylab, side=2, line=1, outer=TRUE, adj=0,
at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}
scatterhist (mu.sample, sigmasq.sample, xlab =  expression(paste("Population Mean   ", mu)), ylab = expression(paste("Population Variance   ", sigma^2)))
dev.off()
#### Gibbs
set.seed(123)
n.burnin = 500
n.iter = 5500
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
# toggle this on to experiment with large n
#y = rnorm(n=1000,mean = 4,sd=2)
n= length(y)
sigmasq.save = rep (NA, n.iter)
mu.save = rep(NA, n.iter)
# hyperparameters for mu:
mu0=0
tausq= 25^2
# hyperparameters for sigmasq:
#a=1000*2 #if these values are large, become informative. prior mean may be approximately equal, but large difference in effects...
#b=1000*1
a=2
b=1
b/(a-1) # prior mean for sigmasq
# note: small values are "uninformative", but still impact estimate for small n
#initial values
mu = 0
sigmasq = 10
for (i in 1:n.iter){
#update mu
# calculate conditional dist of mu given sigmasq and data
# update variance:
sigmasqk = 1 / (n/sigmasq+1/tausq)
# update mean:
meank = (mean(y)/(sigmasq/n) + mu0/tausq)*sigmasqk
mu = rnorm (1, meank, sqrt(sigmasqk))
mu.save[i] = mu
# update sigmasq
# calculate conditional dist of sigmasq given mu and data
RRR = y - mu
sigmasq = rinvgamma(1,shape=n/2+a,scale=sum(RRR^2)/2 + b) # note: the shape parameter does not change; see slide 33
sigmasq.save[i] = sigmasq
}
# estimate of posterior mean for mu:
mu.sample = mu.save[(n.burnin+1):n.iter]
sigmasq.sample = sigmasq.save[(n.burnin+1):n.iter]
mean(mu.sample)
mean(y) # compare to mle for mu
# estimate of posterior mean for sigmasq
mean(sigmasq.sample)
sum((y - mean(y))^2)/n # compare to sample variance, even with uninformative priors, will be different when n is small
par (mfrow = c(2,1))
plot (mu.save, type = "l", xlab = "Iteration",  ylab = expression(mu), cex.lab = 1, col = "blue", main = expression(paste("Population Mean   ", mu)))
plot (sigmasq.save, type = "l", xlab = "Iteration",  ylab = expression(sigma^2), cex.lab = 1, col = "blue", main =  expression(paste("Population Variance   ", sigma^2)) )
scatterhist = function(x, y, xlab="", ylab=""){
zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
xhist = hist(x, plot=FALSE)
yhist = hist(y, plot=FALSE)
top = max(c(xhist$counts, yhist$counts))
par(mar=c(3,3,1,1))
plot(x,y,col = "#0000ff22")
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0)
par(mar=c(3,0,1,1))
barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE)
par(oma=c(3,3,0,0))
mtext(xlab, side=1, line=1, outer=TRUE, adj=0,
at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
mtext(ylab, side=2, line=1, outer=TRUE, adj=0,
at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}
scatterhist (mu.sample, sigmasq.sample, xlab =  expression(paste("Population Mean   ", mu)), ylab = expression(paste("Population Variance   ", sigma^2)))
source('C:/Users/5/Downloads/M8_Rcode_v2.R', echo=TRUE)
### Varying Sigma2
par (mfrow = c(2,2))
sigma2s = c(1, 2, 10, 100)
x.grid = seq(-1, 7, by = 0.0005)
for (i in 1:4){
post.var = 1/(n/sigma2s[i]+1/tau2)
post.mean = post.var*( (n/sigma2s[i])*mean(x) + (1/tau2)*0)
prior.y = dnorm (x.grid, mu0, sqrt(tau2))
post.y = dnorm (x.grid, post.mean, sqrt(post.var))
samp.y = dnorm(x.grid, mean(x), sqrt(sigma2s[i]/n) )
plot (post.y~x.grid, type = "l", xlim = range(x.grid), ylab = "Density", xlab = expression(mu), lwd = 3, cex.lab = 1.4, ylim = c(0, 1.5) )
lines (prior.y~x.grid, type = "l", col = 2, lwd = 3)
lines (samp.y~x.grid, type = "l", col = "blue", lwd = 3)
legend ("topleft", c("Prior", "Posterior", "Sampling"), lwd = 3, col = c(2,1, "blue"), cex=1, bty = "n")
abline (v = mean (x), lty = 3, lwd = 3, col = "blue")
text (mean(x), 0.9, paste("Sample Mean = ",round(mean(x),2)), col = "blue", font = 2, pos = 4)
abline (v = post.mean, lty = 3, lwd = 3)
text (post.mean, 1.4, paste("Posterior Mean = ",round(post.mean,2)), font = 2, pos = 4)
title (main = paste ("Population Variance =", sigma2s[i]) )
}
#####################
# Posterior of sigma^2 given known \mu
### Invgamma
library (MCMCpack)
x.grid = seq(1, 20, by = .05)
r1 = dinvgamma (x.grid, 20, 100)
r2 = dinvgamma (x.grid, 10, 50)
r3 = dinvgamma (x.grid, 1, 5)
r4 = dinvgamma (x.grid, 0.01, 0.05)
plot (r1~x.grid, type = "l", lwd = 3, xlab = "X", ylab = "Density")
lines (r2~x.grid, type = "l", col = "2", lwd = 3)
lines (r3~x.grid, type = "l", col = "3", lwd = 3)
lines (r4~x.grid, type = "l", col = "4", lwd = 3)
legend ("topright", legend = c("a = 20, b = 100", "a = 10, b = 50", "a = 1, b = 5", "a = 0.01, b = 0.05"), col = 1:4, lwd=3, bty = "n", cex  = 1.5)
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
rss = sum( (y - 4)^2 )
n= 10
rss/10 # corresponds to the mle estimate of sigma^2, slide 29
p1 = dinvgamma (x.grid, 10/2 + 20, rss/2 + 100)
p2 = dinvgamma (x.grid, 10/2 + 10, rss/2 + 50)
p3 = dinvgamma (x.grid, 10/2 + 1, rss/2 + 5)
p4 = dinvgamma (x.grid, 10/2 + .01, rss/2 + 0.05)
mle = density( rss*1/rchisq(50000,10) ) # this is turning slide 30 into an equation for sigma^2 rather than \hat{sigma}^2
# or you could use dinvgamma(x.grid,10/2,rss/2)
plot (r1~x.grid, type = "l", lwd = 3, xlab = "X", ylab = "Density", lty = 2, ylim = c(0, .5), xlim = c(1, 10) )
lines (r2~x.grid, type = "l", col = "2", lwd = 3, lty = 3)
lines (r4~x.grid, type = "l", col = "4", lwd = 3, lty = 3)
lines (p1~x.grid, type = "l", col = "1", lwd = 3)
lines (p2~x.grid, type = "l", col = "2", lwd = 3)
lines (p4~x.grid, type = "l", col = "4", lwd = 3)
lines (mle, type = "l", col = "purple", lwd = 3)
legend ("topright", legend = c(paste("Prior: ",c("a = 20, b = 100", "a = 10, b = 50", "a = 0.01, b = 0.05")), "MLE sampling distribution")
, col = c(1,2,4, "purple"), lwd=3, lty = c(3,3,3,1), bty = "n", cex  = 1.2)
legend ("right", legend = c(paste("Posterior Mean: ", c(4.91, 4.85, 4.49)), "MLE sampling mean = 4.47"), col = c(1,2,4, "purple"), lwd=3, lty = 1, bty = "n", cex  = 1.2)
abline(v = 3.572, col = "forestgreen", lwd = 4, lty = 2)
text(3.572, .5, pos = 4, "MLE Estimate = 3.57", col = "forestgreen", font = 2)
set.seed(123)
n.burnin = 500
n.iter = 5500
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
# toggle this on to experiment with large n
#y = rnorm(n=1000,mean = 4,sd=2)
n= length(y)
sigmasq.save = rep (NA, n.iter)
mu.save = rep(NA, n.iter)
# hyperparameters for mu:
mu0=0
tausq= 25^2
#a=1000*2 #if these values are large, become informative. prior mean may be approximately equal, but large difference in effects...
#b=1000*1
a=2
b=1
b/(a-1) # prior mean for sigmasq
# note: small values are "uninformative", but still impact estimate for small n
#initial values
mu = 0
sigmasq = 10
for (i in 1:n.iter){
#update mu
# calculate conditional dist of mu given sigmasq and data
# update variance:
sigmasqk = 1 / (n/sigmasq+1/tausq)
# update mean:
meank = (mean(y)/(sigmasq/n) + mu0/tausq)*sigmasqk
mu = rnorm (1, meank, sqrt(sigmasqk))
mu.save[i] = mu
# update sigmasq
# calculate conditional dist of sigmasq given mu and data
RRR = y - mu
sigmasq = rinvgamma(1,shape=n/2+a,scale=sum(RRR^2)/2 + b) # note: the shape parameter does not change; see slide 33
sigmasq.save[i] = sigmasq
}
mu.sample = mu.save[(n.burnin+1):n.iter]
sigmasq.sample = sigmasq.save[(n.burnin+1):n.iter]
mean(mu.sample)
mean(y) # compare to mle for mu
mean(sigmasq.sample)
sum((y - mean(y))^2)/n # compare to sample variance, even with uninformative priors, will be different when n is small
par (mfrow = c(2,1))
plot (mu.save, type = "l", xlab = "Iteration",  ylab = expression(mu), cex.lab = 1, col = "blue", main = expression(paste("Population Mean   ", mu)))
plot (sigmasq.save, type = "l", xlab = "Iteration",  ylab = expression(sigma^2), cex.lab = 1, col = "blue", main =  expression(paste("Population Variance   ", sigma^2)) )
scatterhist = function(x, y, xlab="", ylab=""){
zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
xhist = hist(x, plot=FALSE)
yhist = hist(y, plot=FALSE)
top = max(c(xhist$counts, yhist$counts))
par(mar=c(3,3,1,1))
plot(x,y,col = "#0000ff22")
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0)
par(mar=c(3,0,1,1))
barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE)
par(oma=c(3,3,0,0))
mtext(xlab, side=1, line=1, outer=TRUE, adj=0,
at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
mtext(ylab, side=2, line=1, outer=TRUE, adj=0,
at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}
scatterhist (mu.sample, sigmasq.sample, xlab =  expression(paste("Population Mean   ", mu)), ylab = expression(paste("Population Variance   ", sigma^2)))
b/(a-1) # prior mean for sigmasq
source('C:/Users/5/Desktop/2nd semester/Bayesian/gibbs.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags for hierarchical model.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
install.packages("rjags")
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/gibbs.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/linear model.R', echo=TRUE)
install.packages("car")
source('C:/Users/5/Desktop/2nd semester/Bayesian/linear model.R', echo=TRUE)
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
n = 100
X = 5*runif(n)
beta0 = 3
beta1 = 1.5 #value used in lecture slides
#beta1 = -0.05 # change to a number near zero for demonstration on transforming quantiles
sigmasq = 1
Y = beta0+beta1*X+rnorm(n)
par(mar=c(2,2,1,1))
plot(Y~X,col='blue',pch=20)
?MCMCregress
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
plot(fit)
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
library(MCMCpack)
?MCMCregress
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
n = 100
X = 5*runif(n)
beta0 = 3
beta1 = 1.5 #value used in lecture slides
#beta1 = -0.05 # change to a number near zero for demonstration on transforming quantiles
sigmasq = 1
Y = beta0+beta1*X+rnorm(n)
par(mar=c(2,2,1,1))
plot(Y~X,col='blue',pch=20)
?MCMCregress
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
plot(fit)
class(fit)
summary(fit)
data1_jags = list(y=X, n=nrow(X))
data1_jags = list(y=X, n=nrow(X))
inits_func = function(){
inits = list('b'=rnorm(2, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = X, inits = inits_func, n.chains = 3)
X = 5*runif(n)
X
mod1 = jags.model(file = textConnection(model_string),
data = X, inits = inits_func, n.chains = 3)
mod1 = jags.model(file = textConnection(model_string),
data = list(X), inits = inits_func, n.chains = 3)
data1_jags = list(y=X, n=nrow(X))
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
data1_jags = list(y=X, n=nrow(X))
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
data1_jags
data1_jags = list(y=X, n=length(X))
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
model_string  = "model{
for(i in 1:n){
y[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]
}
for(j in 1:1){
b[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(5.0/2.0, 5.0*10.0/2.0)
sig2 = 1.0/prec
sig = sqrt(sig2)
}"
set.seed(72)
data1_jags = list(y=X, n=length(X))
inits_func = function(){
inits = list('b'=rnorm(2, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
model_string  = "model{
for(i in 1:n){
y[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]
}
for(j in 1:1){
b[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(5.0/2.0, 5.0*10.0/2.0)
sig2 = 1.0/prec
sig = sqrt(sig2)
}"
set.seed(72)
data1_jags = list(y=X, n=length(X))
inits_func = function(){
inits = list('b'=rnorm(1, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
str(mod1_sim)
plot(mod1_sim)
library('rjags')
model_string  = "model{
for(i in 1:n){
y[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]
}
for(j in 1:1){
b[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(0.1/2.0, 0.01*0.01/2.0)
sig2 = 1.0/prec
sig = sqrt(sig2)
}"
set.seed(72)
data1_jags = list(y=X, n=length(X))
inits_func = function(){
inits = list('b'=rnorm(1, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
str(mod1_sim)
plot(mod1_sim)
#this option assigns more RAM for java to enable high-RAM-comsuming processing
# especially in writing large dataset to .xlxs
options(java.parameters = "-Xmx8048m")
# # install necessary package
library(BiocManager)
# if you have install DESeq2, uncomment the following line
# BiocManager::install("DESeq2")
# BiocManager::install("biomaRt")
library(DESeq2)
library(biomaRt)
library(ggplot2)
library(xlsx)
library(pheatmap)
library(tidyverse)
library(magrittr)
dat <- read_csv('data/Bijean/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
source('E:/data/repos/Kinetics/R/processing/bijean.R', echo=TRUE)
source('E:/data/repos/Kinetics/R/processing/bijean.R', echo=TRUE)
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
# remove the blank column
dat <- dat %>% select(-X70)
# ignore the warning
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
dat <- dat %>% select(-X70)
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
# remove the blank column
dat <- dat %>% dlpyr::select(-X70)
# remove the blank column
dat <- dat %>% dplyr::select(-X70)
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
))
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character()
))
dat
View(dat)
dat <- read_csv('data/BIJEAN RNA SEQ COUNTS.csv', col_types = cols(
.default = col_double(),
Geneid = col_character(),
X70 = col_logical()
))
# remove the blank column
dat <- dat %>% select(-X70)
source('E:/data/repos/Kinetics/R/processing/bijean.R', echo=TRUE)
#----------------------------------------------------------------------------------------
sheetname <- names(result_list)
sheetname <- names(result_list)
for(i in 1:length(result_list)){
write.xlsx(x = result_list[[i]], file = 'output/bijean_result.xlsx',
sheetName = sheetname[i],
append = T)
}
sheetname <- names(result_list)
sheetname
result_list <- c(res.HC_HC_TRANS.HC_RosSep = res.HC_HC_TRANS.HC_RosSep,
res.HC_CF_TRANS.HC_RosSe = res.HC_CF_TRANS.HC_RosSep,
res.CF_HC_TRANS.CF_RosSep = res.CF_HC_TRANS.CF_RosSep,
res.CF_CF_TRANS.CF_RosSep = res.CF_CF_TRANS.CF_RosSep,
res.CF_M0.HC_M0 = res.CF_M0.HC_M0,
res.CF_M1.HC_M1 = res.CF_M1.HC_M1,
res.CF_M2.HC_M2 = res.CF_M2.HC_M2,
res.CF_M17.HC_M17 = res.CF_M17.HC_M17
)
sheetname <- names(result_list)
sheetname
for(i in 1:length(result_list)){
write.xlsx(x = result_list[[i]], file = 'output/bijean_result.xlsx',
sheetName = sheetname[i],
append = T)
}
choose(9, 11)
choose(11, 9)
pnorm(9, 11)
pnorm(10, 11)
pnorm(11, 11)
pbinom(9, 11, 0.5)
pbinom(10, 11, 0.5)
qbinom(10, 11, 0.5)
#----------------------------------------------------------------------------------------
sheetname <- names(result_list)
pbinom(10, 11, 0.5)
