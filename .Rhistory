# set dds condition with all time points
# set 'blood' as base level, the default base level is determined by alphabet order
# other statement if also available for this purpose
# Caution!: the document only give example for factor with two levels, not sure about accuracy for
#   factor with more than 2 levels
dds$condition <- factor(dds$condition, levels = c("2hr_CF", "2hr_LTB4", '8hr_CFminus', '8hr_CFplus'))
# drop levels that with no sample
dds$condition <- droplevels(dds$condition)
res_2hr_LTB4 <- results(dds, contrast = c('condition', "2hr_LTB4",'2hr_CF'))
res_8hr_CFminus <- results(dds, contrast = c('condition', '8hr_CFminus','2hr_CF'))
res_8hr_CFplus <- results(dds, contrast = c('condition', '8hr_CFplus','2hr_CF'))
DESeq(dds)
library(DESeq2)
library(tidyverse)
library(xlsx)
library(magrittr)
#---------------------------------------------------------------------
dat <- read_csv(file = 'data/Trblockade_orgin.csv',
col_types = cols(
`GENE ID` = col_character(),
HD1_2hr_CF = col_double(),
HD1_8hr_CFminus = col_double(),
HD1_8hr_CFplus = col_double(),
HD2_2hr_CF = col_double(),
HD2_2hr_LTB4 = col_double(),
HD2_8hr_CFminus = col_double(),
HD3_2hr_CF = col_double(),
HD3_2hr_LTB4 = col_double(),
HD3_8hr_CFminus = col_double(),
HD3_8hr_CFplus = col_double()
))
# read the blank rows
dat %<>% na.omit()
dat <- dat %>% group_by(`GENE ID`) %>%
summarise_all(.funs = mean)
# check unique
all(!duplicated(dat$`GENE ID`))
#------------------------------------------------------------------------------
# step: create coldata
colnames(dat)
coldata <- data.frame(condition = sub('HD\\d{1}_', '', colnames(dat[-1])),
row.names = colnames(dat[-1]))
# create a integer matrix of count
count_matrix <- as.matrix(dat[,-1])
rownames(count_matrix) <- pull(dat[,1])
mode(count_matrix) <- 'integer'
# check whether condition in coldata is the same as count_matrix
all(rownames(coldata) == colnames(count_matrix))
# -----------------------------------------------------------------------------
#create DESeqDataSet(dds) object, dds is a container for intermediate data
dds <- DESeqDataSetFromMatrix(countData = count_matrix,
colData = coldata,
design = ~ condition)
# pre-filtering
# by removing rows in which there are very few reads, we reduce the memory size of the dds data object,
#   and we increase the speed of the transformation and testing functions within DESeq2
keep <- rowSums(counts(dds)) >= 10
dds <- dds[keep,]
# set dds condition with all time points
# set 'blood' as base level, the default base level is determined by alphabet order
# other statement if also available for this purpose
# Caution!: the document only give example for factor with two levels, not sure about accuracy for
#   factor with more than 2 levels
dds$condition <- factor(dds$condition, levels = c("2hr_CF", "2hr_LTB4", '8hr_CFminus', '8hr_CFplus'))
# drop levels that with no sample
dds$condition <- droplevels(dds$condition)
DESeq(dds)
res_2hr_LTB4 <- results(dds, contrast = c('condition', "2hr_LTB4",'2hr_CF'))
res_8hr_CFminus <- results(dds, contrast = c('condition', '8hr_CFminus','2hr_CF'))
res_8hr_CFplus <- results(dds, contrast = c('condition', '8hr_CFplus','2hr_CF'))
#-------------------
res_2hr_LTB4 <- results(dds, contrast = c('condition', "2hr_LTB4",'2hr_CF'))
dds <- DESeq(dds)
res_2hr_LTB4 <- results(dds, contrast = c('condition', "2hr_LTB4",'2hr_CF'))
res_8hr_CFminus <- results(dds, contrast = c('condition', '8hr_CFminus','2hr_CF'))
res_8hr_CFplus <- results(dds, contrast = c('condition', '8hr_CFplus','2hr_CF'))
res_list_all_trb <- list('res_2hr_LTB4' = res_2hr_LTB4  , 'res_8hr_CFminus' = res_8hr_CFminus,
'res_8hr_CFplus' = res_8hr_CFplus)
{
res_all <- DEresult(res_list_all_trb)
pos <- res_all$gene_name %in% pull(gene_name)
res_trb_sub <- res_all[pos,]
write.xlsx2(x = res_trb_sub, file = 'output/res_trb_sub.xlsx')
}
res_all
source('E:/data/repos/Kinetics/R/kinetics/kinetics.R', echo=TRUE)
{
res_list <-
list(
res_1h = res_1h,
res_2h = res_2h,
res_4h = res_4h,
res_6h = res_6h
)
pos <- res_all$gene_name %in% pull(gene_name)
res_kinetics_sub <- res_all[pos,]
write.xlsx2(x = res_kinetics_sub, file = 'output/res_kinetics_sub.xlsx')
}
{
res_list <-
list(
res_1h = res_1h,
res_2h = res_2h,
res_4h = res_4h,
res_6h = res_6h
)
res_all <- DEresult(res_list)
pos <- res_all$gene_name %in% pull(gene_name)
res_kinetics_sub <- res_all[pos,]
write.xlsx2(x = res_kinetics_sub, file = 'output/res_kinetics_sub.xlsx')
}
source('C:/Users/5/Downloads/M8_Rcode_v2.R', echo=TRUE)
install.packages("MCMCpack")
source('C:/Users/5/Downloads/M8_Rcode_v2.R', echo=TRUE)
#### Gibbs
set.seed(123)
n.burnin = 500
n.iter = 5500
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
# toggle this on to experiment with large n
#y = rnorm(n=1000,mean = 4,sd=2)
n= length(y)
sigmasq.save = rep (NA, n.iter)
mu.save = rep(NA, n.iter)
# hyperparameters for mu:
mu0=0
tausq= 25^2
# hyperparameters for sigmasq:
#a=1000*2 #if these values are large, become informative. prior mean may be approximately equal, but large difference in effects...
#b=1000*1
a=2
b=1
b/(a-1) # prior mean for sigmasq
# note: small values are "uninformative", but still impact estimate for small n
#initial values
mu = 0
sigmasq = 10
for (i in 1:n.iter){
#update mu
# calculate conditional dist of mu given sigmasq and data
# update variance:
sigmasqk = 1 / (n/sigmasq+1/tausq)
# update mean:
meank = (mean(y)/(sigmasq/n) + mu0/tausq)*sigmasqk
mu = rnorm (1, meank, sqrt(sigmasqk))
mu.save[i] = mu
# update sigmasq
# calculate conditional dist of sigmasq given mu and data
RRR = y - mu
sigmasq = rinvgamma(1,shape=n/2+a,scale=sum(RRR^2)/2 + b) # note: the shape parameter does not change; see slide 33
sigmasq.save[i] = sigmasq
}
# estimate of posterior mean for mu:
mu.sample = mu.save[(n.burnin+1):n.iter]
sigmasq.sample = sigmasq.save[(n.burnin+1):n.iter]
mean(mu.sample)
mean(y) # compare to mle for mu
# estimate of posterior mean for sigmasq
mean(sigmasq.sample)
sum((y - mean(y))^2)/n # compare to sample variance, even with uninformative priors, will be different when n is small
par (mfrow = c(2,1))
plot (mu.save, type = "l", xlab = "Iteration",  ylab = expression(mu), cex.lab = 1, col = "blue", main = expression(paste("Population Mean   ", mu)))
plot (sigmasq.save, type = "l", xlab = "Iteration",  ylab = expression(sigma^2), cex.lab = 1, col = "blue", main =  expression(paste("Population Variance   ", sigma^2)) )
scatterhist = function(x, y, xlab="", ylab=""){
zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
xhist = hist(x, plot=FALSE)
yhist = hist(y, plot=FALSE)
top = max(c(xhist$counts, yhist$counts))
par(mar=c(3,3,1,1))
plot(x,y,col = "#0000ff22")
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0)
par(mar=c(3,0,1,1))
barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE)
par(oma=c(3,3,0,0))
mtext(xlab, side=1, line=1, outer=TRUE, adj=0,
at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
mtext(ylab, side=2, line=1, outer=TRUE, adj=0,
at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}
scatterhist (mu.sample, sigmasq.sample, xlab =  expression(paste("Population Mean   ", mu)), ylab = expression(paste("Population Variance   ", sigma^2)))
dev.off()
#### Gibbs
set.seed(123)
n.burnin = 500
n.iter = 5500
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
# toggle this on to experiment with large n
#y = rnorm(n=1000,mean = 4,sd=2)
n= length(y)
sigmasq.save = rep (NA, n.iter)
mu.save = rep(NA, n.iter)
# hyperparameters for mu:
mu0=0
tausq= 25^2
# hyperparameters for sigmasq:
#a=1000*2 #if these values are large, become informative. prior mean may be approximately equal, but large difference in effects...
#b=1000*1
a=2
b=1
b/(a-1) # prior mean for sigmasq
# note: small values are "uninformative", but still impact estimate for small n
#initial values
mu = 0
sigmasq = 10
for (i in 1:n.iter){
#update mu
# calculate conditional dist of mu given sigmasq and data
# update variance:
sigmasqk = 1 / (n/sigmasq+1/tausq)
# update mean:
meank = (mean(y)/(sigmasq/n) + mu0/tausq)*sigmasqk
mu = rnorm (1, meank, sqrt(sigmasqk))
mu.save[i] = mu
# update sigmasq
# calculate conditional dist of sigmasq given mu and data
RRR = y - mu
sigmasq = rinvgamma(1,shape=n/2+a,scale=sum(RRR^2)/2 + b) # note: the shape parameter does not change; see slide 33
sigmasq.save[i] = sigmasq
}
# estimate of posterior mean for mu:
mu.sample = mu.save[(n.burnin+1):n.iter]
sigmasq.sample = sigmasq.save[(n.burnin+1):n.iter]
mean(mu.sample)
mean(y) # compare to mle for mu
# estimate of posterior mean for sigmasq
mean(sigmasq.sample)
sum((y - mean(y))^2)/n # compare to sample variance, even with uninformative priors, will be different when n is small
par (mfrow = c(2,1))
plot (mu.save, type = "l", xlab = "Iteration",  ylab = expression(mu), cex.lab = 1, col = "blue", main = expression(paste("Population Mean   ", mu)))
plot (sigmasq.save, type = "l", xlab = "Iteration",  ylab = expression(sigma^2), cex.lab = 1, col = "blue", main =  expression(paste("Population Variance   ", sigma^2)) )
scatterhist = function(x, y, xlab="", ylab=""){
zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
xhist = hist(x, plot=FALSE)
yhist = hist(y, plot=FALSE)
top = max(c(xhist$counts, yhist$counts))
par(mar=c(3,3,1,1))
plot(x,y,col = "#0000ff22")
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0)
par(mar=c(3,0,1,1))
barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE)
par(oma=c(3,3,0,0))
mtext(xlab, side=1, line=1, outer=TRUE, adj=0,
at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
mtext(ylab, side=2, line=1, outer=TRUE, adj=0,
at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}
scatterhist (mu.sample, sigmasq.sample, xlab =  expression(paste("Population Mean   ", mu)), ylab = expression(paste("Population Variance   ", sigma^2)))
source('C:/Users/5/Downloads/M8_Rcode_v2.R', echo=TRUE)
### Varying Sigma2
par (mfrow = c(2,2))
sigma2s = c(1, 2, 10, 100)
x.grid = seq(-1, 7, by = 0.0005)
for (i in 1:4){
post.var = 1/(n/sigma2s[i]+1/tau2)
post.mean = post.var*( (n/sigma2s[i])*mean(x) + (1/tau2)*0)
prior.y = dnorm (x.grid, mu0, sqrt(tau2))
post.y = dnorm (x.grid, post.mean, sqrt(post.var))
samp.y = dnorm(x.grid, mean(x), sqrt(sigma2s[i]/n) )
plot (post.y~x.grid, type = "l", xlim = range(x.grid), ylab = "Density", xlab = expression(mu), lwd = 3, cex.lab = 1.4, ylim = c(0, 1.5) )
lines (prior.y~x.grid, type = "l", col = 2, lwd = 3)
lines (samp.y~x.grid, type = "l", col = "blue", lwd = 3)
legend ("topleft", c("Prior", "Posterior", "Sampling"), lwd = 3, col = c(2,1, "blue"), cex=1, bty = "n")
abline (v = mean (x), lty = 3, lwd = 3, col = "blue")
text (mean(x), 0.9, paste("Sample Mean = ",round(mean(x),2)), col = "blue", font = 2, pos = 4)
abline (v = post.mean, lty = 3, lwd = 3)
text (post.mean, 1.4, paste("Posterior Mean = ",round(post.mean,2)), font = 2, pos = 4)
title (main = paste ("Population Variance =", sigma2s[i]) )
}
#####################
# Posterior of sigma^2 given known \mu
### Invgamma
library (MCMCpack)
x.grid = seq(1, 20, by = .05)
r1 = dinvgamma (x.grid, 20, 100)
r2 = dinvgamma (x.grid, 10, 50)
r3 = dinvgamma (x.grid, 1, 5)
r4 = dinvgamma (x.grid, 0.01, 0.05)
plot (r1~x.grid, type = "l", lwd = 3, xlab = "X", ylab = "Density")
lines (r2~x.grid, type = "l", col = "2", lwd = 3)
lines (r3~x.grid, type = "l", col = "3", lwd = 3)
lines (r4~x.grid, type = "l", col = "4", lwd = 3)
legend ("topright", legend = c("a = 20, b = 100", "a = 10, b = 50", "a = 1, b = 5", "a = 0.01, b = 0.05"), col = 1:4, lwd=3, bty = "n", cex  = 1.5)
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
rss = sum( (y - 4)^2 )
n= 10
rss/10 # corresponds to the mle estimate of sigma^2, slide 29
p1 = dinvgamma (x.grid, 10/2 + 20, rss/2 + 100)
p2 = dinvgamma (x.grid, 10/2 + 10, rss/2 + 50)
p3 = dinvgamma (x.grid, 10/2 + 1, rss/2 + 5)
p4 = dinvgamma (x.grid, 10/2 + .01, rss/2 + 0.05)
mle = density( rss*1/rchisq(50000,10) ) # this is turning slide 30 into an equation for sigma^2 rather than \hat{sigma}^2
# or you could use dinvgamma(x.grid,10/2,rss/2)
plot (r1~x.grid, type = "l", lwd = 3, xlab = "X", ylab = "Density", lty = 2, ylim = c(0, .5), xlim = c(1, 10) )
lines (r2~x.grid, type = "l", col = "2", lwd = 3, lty = 3)
lines (r4~x.grid, type = "l", col = "4", lwd = 3, lty = 3)
lines (p1~x.grid, type = "l", col = "1", lwd = 3)
lines (p2~x.grid, type = "l", col = "2", lwd = 3)
lines (p4~x.grid, type = "l", col = "4", lwd = 3)
lines (mle, type = "l", col = "purple", lwd = 3)
legend ("topright", legend = c(paste("Prior: ",c("a = 20, b = 100", "a = 10, b = 50", "a = 0.01, b = 0.05")), "MLE sampling distribution")
, col = c(1,2,4, "purple"), lwd=3, lty = c(3,3,3,1), bty = "n", cex  = 1.2)
legend ("right", legend = c(paste("Posterior Mean: ", c(4.91, 4.85, 4.49)), "MLE sampling mean = 4.47"), col = c(1,2,4, "purple"), lwd=3, lty = 1, bty = "n", cex  = 1.2)
abline(v = 3.572, col = "forestgreen", lwd = 4, lty = 2)
text(3.572, .5, pos = 4, "MLE Estimate = 3.57", col = "forestgreen", font = 2)
set.seed(123)
n.burnin = 500
n.iter = 5500
y = c(4.2, 6.6, 5.1, 2.0, 2.8, 3.2, 4.7, 4.1, 7.3, 0.8)
# toggle this on to experiment with large n
#y = rnorm(n=1000,mean = 4,sd=2)
n= length(y)
sigmasq.save = rep (NA, n.iter)
mu.save = rep(NA, n.iter)
# hyperparameters for mu:
mu0=0
tausq= 25^2
#a=1000*2 #if these values are large, become informative. prior mean may be approximately equal, but large difference in effects...
#b=1000*1
a=2
b=1
b/(a-1) # prior mean for sigmasq
# note: small values are "uninformative", but still impact estimate for small n
#initial values
mu = 0
sigmasq = 10
for (i in 1:n.iter){
#update mu
# calculate conditional dist of mu given sigmasq and data
# update variance:
sigmasqk = 1 / (n/sigmasq+1/tausq)
# update mean:
meank = (mean(y)/(sigmasq/n) + mu0/tausq)*sigmasqk
mu = rnorm (1, meank, sqrt(sigmasqk))
mu.save[i] = mu
# update sigmasq
# calculate conditional dist of sigmasq given mu and data
RRR = y - mu
sigmasq = rinvgamma(1,shape=n/2+a,scale=sum(RRR^2)/2 + b) # note: the shape parameter does not change; see slide 33
sigmasq.save[i] = sigmasq
}
mu.sample = mu.save[(n.burnin+1):n.iter]
sigmasq.sample = sigmasq.save[(n.burnin+1):n.iter]
mean(mu.sample)
mean(y) # compare to mle for mu
mean(sigmasq.sample)
sum((y - mean(y))^2)/n # compare to sample variance, even with uninformative priors, will be different when n is small
par (mfrow = c(2,1))
plot (mu.save, type = "l", xlab = "Iteration",  ylab = expression(mu), cex.lab = 1, col = "blue", main = expression(paste("Population Mean   ", mu)))
plot (sigmasq.save, type = "l", xlab = "Iteration",  ylab = expression(sigma^2), cex.lab = 1, col = "blue", main =  expression(paste("Population Variance   ", sigma^2)) )
scatterhist = function(x, y, xlab="", ylab=""){
zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
xhist = hist(x, plot=FALSE)
yhist = hist(y, plot=FALSE)
top = max(c(xhist$counts, yhist$counts))
par(mar=c(3,3,1,1))
plot(x,y,col = "#0000ff22")
par(mar=c(0,3,1,1))
barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0)
par(mar=c(3,0,1,1))
barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE)
par(oma=c(3,3,0,0))
mtext(xlab, side=1, line=1, outer=TRUE, adj=0,
at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
mtext(ylab, side=2, line=1, outer=TRUE, adj=0,
at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}
scatterhist (mu.sample, sigmasq.sample, xlab =  expression(paste("Population Mean   ", mu)), ylab = expression(paste("Population Variance   ", sigma^2)))
b/(a-1) # prior mean for sigmasq
source('C:/Users/5/Desktop/2nd semester/Bayesian/gibbs.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags for hierarchical model.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
install.packages("rjags")
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/gibbs.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/jags.R', echo=TRUE)
source('C:/Users/5/Desktop/2nd semester/Bayesian/linear model.R', echo=TRUE)
install.packages("car")
source('C:/Users/5/Desktop/2nd semester/Bayesian/linear model.R', echo=TRUE)
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
n = 100
X = 5*runif(n)
beta0 = 3
beta1 = 1.5 #value used in lecture slides
#beta1 = -0.05 # change to a number near zero for demonstration on transforming quantiles
sigmasq = 1
Y = beta0+beta1*X+rnorm(n)
par(mar=c(2,2,1,1))
plot(Y~X,col='blue',pch=20)
?MCMCregress
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
plot(fit)
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
library(MCMCpack)
?MCMCregress
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
n = 100
X = 5*runif(n)
beta0 = 3
beta1 = 1.5 #value used in lecture slides
#beta1 = -0.05 # change to a number near zero for demonstration on transforming quantiles
sigmasq = 1
Y = beta0+beta1*X+rnorm(n)
par(mar=c(2,2,1,1))
plot(Y~X,col='blue',pch=20)
?MCMCregress
fit = MCMCregress (Y~X, b0 = c(0,0), B0 = 0.001^2, c0=0.01, d0=0.01)
plot(fit)
class(fit)
summary(fit)
data1_jags = list(y=X, n=nrow(X))
data1_jags = list(y=X, n=nrow(X))
inits_func = function(){
inits = list('b'=rnorm(2, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = X, inits = inits_func, n.chains = 3)
X = 5*runif(n)
X
mod1 = jags.model(file = textConnection(model_string),
data = X, inits = inits_func, n.chains = 3)
mod1 = jags.model(file = textConnection(model_string),
data = list(X), inits = inits_func, n.chains = 3)
data1_jags = list(y=X, n=nrow(X))
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
data1_jags = list(y=X, n=nrow(X))
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
data1_jags
data1_jags = list(y=X, n=length(X))
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
model_string  = "model{
for(i in 1:n){
y[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]
}
for(j in 1:1){
b[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(5.0/2.0, 5.0*10.0/2.0)
sig2 = 1.0/prec
sig = sqrt(sig2)
}"
set.seed(72)
data1_jags = list(y=X, n=length(X))
inits_func = function(){
inits = list('b'=rnorm(2, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
model_string  = "model{
for(i in 1:n){
y[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]
}
for(j in 1:1){
b[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(5.0/2.0, 5.0*10.0/2.0)
sig2 = 1.0/prec
sig = sqrt(sig2)
}"
set.seed(72)
data1_jags = list(y=X, n=length(X))
inits_func = function(){
inits = list('b'=rnorm(1, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
str(mod1_sim)
plot(mod1_sim)
library('rjags')
model_string  = "model{
for(i in 1:n){
y[i] ~ dnorm(mu[i], prec)
mu[i] = b[1]
}
for(j in 1:1){
b[j] ~ dnorm(0.0, 1.0/1.0e6)
}
prec ~ dgamma(0.1/2.0, 0.01*0.01/2.0)
sig2 = 1.0/prec
sig = sqrt(sig2)
}"
set.seed(72)
data1_jags = list(y=X, n=length(X))
inits_func = function(){
inits = list('b'=rnorm(1, 0.0,100.0), 'prec'=rgamma(1, 1.0, 1.0))
}
mod1 = jags.model(file = textConnection(model_string),
data = data1_jags, inits = inits_func, n.chains = 3)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
update(mod1, 1000)
mod1_sim = coda.samples(model = mod1,
variable.names = c('b', 'sig'),n.iter = 5000)
str(mod1_sim)
plot(mod1_sim)
